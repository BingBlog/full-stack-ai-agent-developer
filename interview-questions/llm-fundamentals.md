# LLM 与模型基础

## 基础原理与架构

1. [频率:高][难度:易] Transformer 相比传统 RNN/CNN 的核心优势是什么，为什么更适合长序列建模？ 提示：从并行化、全局感受野、梯度传播角度分析。
2. [频率:高][难度:易] 解释 Encoder-Decoder 架构与 Decoder-only 架构的差异，各自适用于哪些任务？ 提示：关注输入输出对齐、对话生成与翻译等场景。
3. [频率:高][难度:中] 什么是残差连接与 LayerNorm，它们如何帮助稳定深层网络训练？ 提示：强调梯度流动和参数尺度控制。
4. [频率:高][难度:中] 为什么现代大模型常使用 GELU/SiLU 激活函数而非 ReLU？ 提示：讨论平滑性、对称性与训练稳定性。
5. [频率:中][难度:中] 说明 Position-wise Feed-Forward Network 的作用以及宽度设置对模型能力的影响。 提示：联想到瓶颈层与参数规模。
6. [频率:中][难度:难] 对比词级别、子词级别与字节级别分词策略，LLM 在中文场景下通常如何选择？ 提示：考虑语义完整性、词表大小与训练成本。
7. [频率:中][难度:难] 当模型规模和上下文窗口扩展时，训练数据与算力需求如何变化？ 提示：引用 scaling law、Chinchilla 最优点。

## 注意力机制与训练优化

1. [频率:高][难度:易] 请推导 Scaled Dot-Product Attention 的计算流程，并说明缩放因子的作用。 提示：关注内积数值范围与 softmax 梯度。
2. [频率:高][难度:易] Multi-Head Attention 为何能提升模型表达能力？ 提示：从子空间特征与并行头的角度阐述。
3. [频率:高][难度:中] 自注意力的 O(n²) 复杂度如何通过 FlashAttention 或滑动窗口等方式降低？ 提示：比较计算重排与稀疏模式。
4. [频率:高][难度:中] 什么是交叉注意力（Cross-Attention），在多模态或工具调用中如何使用？ 提示：强调 Query/Key 来自不同源。
5. [频率:中][难度:中] 训练稳定性中常见的梯度爆炸/消失问题如何通过学习率预热与梯度裁剪缓解？ 提示：结合 warmup、AdamW 设置。
6. [频率:中][难度:难] 解释 DeepNorm、Pre-Norm、Post-Norm 的差异以及对深层 Transformer 的影响。 提示：关注残差比例与训练深度。
7. [频率:中][难度:难] 为什么需要引入正则化手段（如 Dropout、Label Smoothing）？在大模型中如何设置？ 提示：讨论过拟合与泛化。

## 微调与适配技术

1. [频率:高][难度:易] 对比全量微调、Adapter、LoRA、Prefix Tuning 的优缺点。 提示：关注参数开销与效果。
2. [频率:高][难度:易] 解释 LoRA 的低秩分解思想，并说明 r、α 选择与 target modules 选取的依据。 提示：结合权重更新限制。
3. [频率:高][难度:中] QLoRA 如何结合 NF4 量化与双量化，再配合 LoRA 达到低资源微调？ 提示：说明量化权重与梯度存储策略。
4. [频率:高][难度:中] 什么是 PEFT 框架，它如何帮助你管理不同下游任务的 LoRA 权重？ 提示：讨论模块化与合并流程。
5. [频率:中][难度:中] 指令微调中如何构造多样化的指令/反馈数据以减少幻觉？ 提示：参考 Self-Instruct、多人标注策略。
6. [频率:中][难度:难] 如何评估微调后模型是否忘记原始能力（catastrophic forgetting）？ 提示：考虑保留测试集与对比实验。
7. [频率:中][难度:难] 如果需要将多语言支持加入已有模型，你会选择哪种参数高效方法？ 提示：结合多语言 Adapter、LoRA 或 embedding 扩展。

## 推理与性能优化

1. [频率:高][难度:中] KV Cache 的原理是什么，如何优化批量推理中的缓存复用？ 提示：从自回归生成步骤讲起。
2. [频率:中][难度:中] 解释连续批处理（Continuous Batching）如何提升吞吐，并在哪些引擎中实现。 提示：提及 vLLM、TGI、SGLang 等。
3. [频率:中][难度:难] 不同量化策略（GPTQ、AWQ、FP8）在精度与性能之间有何权衡？ 提示：关注离线量化与在线校准。
4. [频率:中][难度:难] 当上下文超过模型窗口时，可采用哪些扩展策略（RoPE scaling、窗口注意力等）？ 提示：讨论长上下文适配与性能限制。
5. [频率:低][难度:难] Beam Search、Sampling、Top-k、Top-p 在生成质量与多样性上的差异是什么？ 提示：给出应用场景与参数选择。
6. [频率:低][难度:难] 为何要使用 Speculative Decoding 或 EAGLE 加速推理，它的工作机制如何？ 提示：强调草稿模型与验证模型。
7. [频率:低][难度:难] GPU/CPU 混合推理时需要注意哪些内存与带宽瓶颈？ 提示：结合张量并行、流水线并行以及 ZeRO 等策略。

## 部署与工具链

1. [频率:高][难度:易] 对比 vLLM、TensorRT-LLM、Inferentia 等推理方案，你会如何选择？ 提示：围绕硬件兼容、吞吐、延迟。
2. [频率:中][难度:易] 如何使用 Hugging Face Optimum 或 DeepSpeed-Inference 构建轻量部署流程？ 提示：讨论模型编译与权重转换。
3. [频率:中][难度:中] 说明如何将 LLM 接入 Ray Serve 或 KServe，实现多模型治理。 提示：涉及路由、负载均衡、监控。
4. [频率:中][难度:中] 在 CI/CD 中管理大模型权重需要哪些版本控制和校验措施？ 提示：考虑模型仓库、哈希校验、Artifact 管理。
5. [频率:低][难度:中] 如何设计多租户隔离，防止不同业务共享模型时互相干扰？ 提示：谈容器隔离、资源配额与鉴权。
6. [频率:低][难度:难] 在成本敏感场景下，如何利用 Serverless GPU 或 Spot 实例部署推理？ 提示：评估弹性伸缩和冷启动。
7. [频率:低][难度:难] 将模型部署到边缘或移动端时，常见的蒸馏与剪枝策略有哪些？ 提示：参考知识蒸馏、结构化剪枝。

## 评估与监控

1. [频率:高][难度:易] 如何构建 LLM 评估基准，包括准确性、相关性、真实性指标？ 提示：举例 MT-Bench、MMLU、BLEU 等。
2. [频率:中][难度:易] 解释基于人类反馈的评估方法（如 A/B 测试、Human Preference）与自动评估的差异。 提示：关注成本与可信度。
3. [频率:中][难度:中] 当模型出现幻觉时，如何设计专项评估集与指标监控？ 提示：结合红队测试与自动判断。
4. [频率:中][难度:中] 构建日志追踪系统时，需要保留哪些上下文信息以便复现问题？ 提示：提及 Prompt、模型版本、温度等配置。
5. [频率:低][难度:中] 说明如何在生产中实现持续评估（continuous evaluation），保障模型质量。 提示：考虑离线回放与在线采样。
6. [频率:低][难度:难] LangSmith、Weights & Biases、Arize Phoenix 等平台提供哪些监控能力？ 提示：比较追踪、可视化、警报。
7. [频率:低][难度:难] 模型评估中为什么需要组合主观指标与客观指标？ 提示：强调业务目标与用户体验权衡。

## 案例与问题处理

1. [频率:高][难度:中] 分享一次模型输出偏离预期的案例，你如何定位并修复？ 提示：说明数据、Prompt、参数排查顺序。
2. [频率:中][难度:中] 当训练数据存在偏见时，你采取了哪些策略减少不当输出？ 提示：讨论数据清洗、对齐训练、规则过滤。
3. [频率:中][难度:难] 遇到推理延迟突增时，你如何分析链路并找到根因？ 提示：从网络、模型、存储层面拆解。
4. [频率:中][难度:难] 在 GPU 资源紧张时，如何安排微调或推理任务以避免冲突？ 提示：谈资源编排、优先级、配额管理。
5. [频率:中][难度:难] 描述一次你将开源模型成功上线到生产的经历，关键挑战是什么？ 提示：关注评估、性能、合规流程。
6. [频率:低][难度:难] 当模型升级导致兼容性问题时，你如何制定回滚与验证计划？ 提示：提到灰度发布、版本对比。
7. [频率:低][难度:难] 面对突发的安全漏洞或模型泄露风险，你会如何响应？ 提示：强调密钥撤销、审计、法律合规。
8. [频率:低][难度:难] 结合业务案例说明如何衡量模型带来的业务价值与 ROI。 提示：举例转化率、效率提升或成本节省。
